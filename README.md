# Types of classification

This repository offers a comprehensive exploration of various classification methods along with their implementations in Jupyter notebooks. Classification is a fundamental task in machine learning aimed at categorizing input data into predefined classes or labels. These notebooks serve as a guide to understanding different classification algorithms and their applications.

## Classification Methods Covered
------------------------------

1.  **Logistic Regression**
    
    *   Logistic Regression is a linear classification algorithm used for binary classification tasks. It models the probability that a given input belongs to a particular class using a logistic function.
    *   Notebook: `Logistic_Regression.ipynb`
2.  **Decision Trees**
    
    *   Decision Trees are non-linear classification algorithms that partition the feature space into regions based on feature values. They make predictions by traversing the tree from the root to a leaf node.
    *   Notebook: `Decision_Trees.ipynb`
3.  **Random Forest**
    
    *   Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or the average prediction (regression) of the individual trees.
    *   Notebook: `Random_Forest.ipynb`
4.  **Support Vector Machines (SVM)**
    
    *   SVM is a powerful classification algorithm that finds the optimal hyperplane that separates classes in the feature space with the maximum margin. It can handle both linear and non-linear classification tasks.
    *   Notebook: `Support_Vector_Machines.ipynb`
5.  **K-Nearest Neighbors (KNN)**
    
    *   KNN is a simple yet effective classification algorithm that classifies a data point based on the majority class of its nearest neighbors in the feature space.
    *   Notebook: `K_Nearest_Neighbors.ipynb`
6.  **Naive Bayes**
    
    *   Naive Bayes is a simple yet effective classification algorithm that classifies a data point. Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. .
    *   Notebook: `Naive_Bayes.ipynb`
7.  **Gradient Boosting Trees**
    
    *   Boosting combines weak learners (usually decision trees with only one split, called decision stumps) sequentially, so that each new tree corrects the errors of the previous one. .
    *   Notebook: `Gradient_Boosting_Trees.ipynb`
8. **MultiLabel Classification**
    *   We also work on how to handle multilabel classification problem. i.e the problems which can have more than one class label for each row of data. 
    *   Notebook: `multilabel_classification.ipynb`


