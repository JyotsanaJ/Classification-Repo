{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GBM\n",
    "* XGBoost\n",
    "* LightGBM\n",
    "* CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting -\n",
    "GB is a machine learning ensemble technique that combines the predictions of multiple weak learners, typically decision trees, sequentially.\n",
    "* before feeding the observations to M2, what we do is update the weights of the observations which are wrongly classified\n",
    "* (probability of selecting a wrongly classified data increases)\n",
    "* regression trees are fit on the negative gradient of the loss function.\n",
    "* Since trees are added sequentially, boosting algorithms learn slowly. In statistical learning, models that learn slowly perform better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting\n",
    "* Gradient Boosting updates the weights by computing the negative gradient of the loss function with respect to the predicted output.\n",
    "* Gradient Boosting can use a wide range of base learners, such as decision trees, and linear models.\n",
    "* Gradient Boosting is generally more robust, as it updates the weights based on the gradients, which are less sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    \"GradientBoostingClassifier\", - \n",
    "*    \"GradientBoostingRegressor\",\n",
    "*    \"AdaBoostClassifier\", - New models with updated weights on initial dataset\n",
    "*    \"AdaBoostRegressor\",\n",
    "*    \"HistGradientBoostingClassifier\",\n",
    "*    \"HistGradientBoostingRegressor\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "# 2 classes\n",
    "X,y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64), (1437,), (360,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state=42) \n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 3, 7, 8, 1, 5, 2, 5, 2, 1, 4, 4, 0, 4, 2, 3, 7, 8, 8, 4, 3,\n",
       "       9, 7, 5, 6, 3, 5, 6, 3, 4, 9, 1, 4, 4, 6, 9, 4, 7, 6, 6, 9, 1, 3,\n",
       "       6, 1, 3, 0, 6, 5, 5, 1, 9, 5, 6, 0, 9, 0, 0, 1, 0, 4, 5, 2, 4, 5,\n",
       "       7, 0, 7, 5, 9, 3, 5, 4, 7, 0, 4, 5, 9, 9, 9, 0, 2, 3, 8, 0, 6, 4,\n",
       "       4, 9, 1, 2, 8, 3, 5, 2, 9, 1, 4, 4, 4, 3, 5, 3, 1, 8, 5, 9, 4, 2,\n",
       "       7, 7, 4, 4, 1, 9, 2, 7, 8, 7, 2, 6, 9, 4, 0, 7, 2, 7, 5, 8, 7, 5,\n",
       "       7, 9, 0, 6, 6, 4, 2, 8, 0, 9, 4, 6, 1, 9, 6, 9, 0, 1, 9, 6, 6, 0,\n",
       "       6, 4, 2, 9, 3, 7, 7, 2, 9, 0, 0, 5, 8, 6, 5, 7, 9, 8, 4, 2, 1, 3,\n",
       "       7, 7, 2, 2, 3, 9, 8, 0, 3, 8, 2, 5, 6, 9, 9, 4, 1, 5, 4, 2, 3, 6,\n",
       "       4, 8, 5, 9, 5, 7, 8, 9, 4, 8, 1, 5, 4, 4, 9, 6, 1, 8, 6, 0, 4, 5,\n",
       "       2, 7, 4, 6, 4, 5, 6, 0, 3, 2, 3, 6, 7, 1, 9, 1, 4, 7, 6, 5, 1, 5,\n",
       "       5, 1, 0, 2, 8, 8, 7, 7, 7, 6, 2, 2, 2, 3, 4, 8, 8, 3, 6, 0, 3, 7,\n",
       "       8, 0, 1, 0, 4, 5, 1, 5, 3, 6, 0, 4, 1, 0, 0, 3, 6, 5, 9, 7, 3, 5,\n",
       "       9, 9, 9, 8, 5, 3, 3, 2, 0, 5, 8, 3, 4, 0, 2, 7, 6, 4, 3, 4, 5, 0,\n",
       "       5, 2, 1, 3, 1, 4, 1, 1, 7, 0, 1, 5, 2, 1, 2, 8, 7, 0, 6, 4, 8, 8,\n",
       "       5, 1, 8, 4, 5, 8, 7, 9, 8, 6, 0, 6, 2, 0, 7, 9, 8, 9, 5, 2, 7, 7,\n",
       "       9, 3, 7, 4, 3, 8, 3, 9])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(loss=\"log_loss\", # log_loss, deviance, exponential\n",
    "                                learning_rate=0.05,\n",
    "                                n_estimators=10,\n",
    "                                subsample=1.0,\n",
    "                                criterion=\"friedman_mse\", # friedman_mse, squared_error\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0.0,\n",
    "                                max_depth=3,\n",
    "                                min_impurity_decrease=0.0,\n",
    "                                init=None,\n",
    "                                random_state=42,\n",
    "                                max_features=\"log2\", # log2, auto,sqrt\n",
    "                                verbose=0,\n",
    "                                max_leaf_nodes=None,\n",
    "                                warm_start=True,\n",
    "                                validation_fraction=0.1,\n",
    "                                n_iter_no_change=None,\n",
    "                                tol=1e-4,\n",
    "                                ccp_alpha=0.0)\n",
    "\n",
    "# Fit to training set\n",
    "gbc.fit(X_train, y_train)\n",
    " \n",
    "# Predict on test set\n",
    "prediction = gbc.predict(X_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 3, 7, 2, 1, 5, 2, 5, 2, 1, 9, 4, 0, 4, 2, 3, 7, 8, 8, 4, 3,\n",
       "       9, 7, 5, 6, 3, 5, 6, 3, 4, 9, 1, 4, 4, 6, 9, 4, 7, 6, 6, 9, 1, 3,\n",
       "       6, 1, 3, 0, 6, 5, 5, 1, 9, 5, 6, 0, 9, 0, 0, 1, 0, 4, 5, 2, 4, 5,\n",
       "       7, 0, 7, 5, 9, 5, 5, 4, 7, 0, 4, 5, 5, 9, 9, 0, 2, 3, 8, 0, 6, 4,\n",
       "       4, 9, 1, 2, 8, 3, 5, 2, 9, 0, 4, 4, 4, 3, 5, 3, 1, 3, 5, 9, 4, 2,\n",
       "       7, 7, 4, 4, 1, 9, 2, 7, 8, 7, 2, 6, 9, 4, 0, 7, 2, 7, 5, 8, 7, 5,\n",
       "       7, 7, 0, 6, 6, 4, 2, 8, 0, 9, 4, 6, 9, 9, 6, 9, 0, 3, 5, 6, 6, 0,\n",
       "       6, 4, 3, 9, 3, 9, 7, 2, 9, 0, 4, 5, 3, 6, 5, 9, 9, 8, 4, 2, 1, 3,\n",
       "       7, 7, 2, 2, 3, 9, 8, 0, 3, 2, 2, 5, 6, 9, 9, 4, 1, 5, 4, 2, 3, 6,\n",
       "       4, 8, 5, 9, 5, 7, 8, 9, 4, 8, 1, 5, 4, 4, 9, 6, 1, 8, 6, 0, 4, 5,\n",
       "       2, 7, 4, 6, 4, 5, 6, 0, 3, 2, 3, 6, 7, 1, 5, 1, 4, 7, 6, 8, 8, 5,\n",
       "       5, 1, 6, 2, 8, 8, 9, 9, 7, 6, 2, 2, 2, 3, 4, 8, 8, 3, 6, 0, 9, 7,\n",
       "       7, 0, 1, 0, 4, 5, 1, 5, 3, 6, 0, 4, 1, 0, 0, 3, 6, 5, 9, 7, 3, 5,\n",
       "       5, 9, 9, 8, 5, 3, 3, 2, 0, 5, 8, 3, 4, 0, 2, 4, 6, 4, 3, 4, 5, 0,\n",
       "       5, 2, 1, 3, 1, 4, 1, 1, 7, 0, 1, 5, 2, 1, 2, 8, 7, 0, 6, 4, 8, 8,\n",
       "       5, 1, 8, 4, 5, 8, 7, 9, 8, 5, 0, 6, 2, 0, 7, 9, 8, 9, 5, 2, 7, 7,\n",
       "       1, 8, 7, 4, 3, 8, 3, 5])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_test == prediction) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.max_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((331, 10), (111, 10), (331,), (111,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
    "                                                    test_size = 0.25, \n",
    "                                                    random_state = 23)\n",
    "train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([283.88578911, 103.60458892, 151.30595716,  88.41778663,\n",
       "       149.5224744 , 154.7324027 , 200.72221524, 159.63019406,\n",
       "       206.11411364, 211.33836085, 124.96244952, 121.57916732,\n",
       "       163.19006777,  83.67545538, 114.9991849 , 211.18534265,\n",
       "       251.90425203, 164.72844538, 129.46897752, 100.21991019,\n",
       "       224.83614726, 159.51225101, 118.45875952, 148.59044144,\n",
       "       132.20830552, 153.34353502, 134.80697867, 130.20754792,\n",
       "       309.93276796,  92.06416738, 149.06351918, 262.24233467,\n",
       "       203.25513475,  75.9403632 , 192.39799314, 248.54137892,\n",
       "       123.26850847, 146.61054564, 153.59277799, 105.94786293,\n",
       "       153.89825258, 210.47625413,  80.44656692, 132.17722462,\n",
       "        80.83352621, 231.40685688, 222.82515018,  70.17964747,\n",
       "       200.6056077 , 100.30364518, 167.85944652,  96.84430375,\n",
       "       221.94263066, 162.09417355, 210.3989053 ,  88.28049319,\n",
       "       249.56959098,  84.06701545, 160.71408909, 233.1350084 ,\n",
       "       201.3103988 , 173.72525384,  92.59623727, 177.99563294,\n",
       "       164.18545226, 110.13938516, 156.90406918, 144.30586286,\n",
       "       239.53042775, 205.27926824, 104.3546287 , 107.53138838,\n",
       "       252.88110562,  77.58717128, 162.50745353, 124.07785481,\n",
       "       270.4358067 , 136.06055306,  85.04613087, 102.01065616,\n",
       "       237.94921772, 129.91948357, 218.44871038, 260.58504032,\n",
       "       166.47457129, 183.12172172, 136.27407883, 142.46582381,\n",
       "       232.7354052 , 151.33816419, 149.81711437, 199.01054027,\n",
       "       204.99952351, 143.1046231 ,  73.28488934, 176.49200729,\n",
       "       170.00577407, 253.87010323, 211.74358682, 169.28763014,\n",
       "       139.00273016, 111.7180299 , 253.43955583,  71.95581571,\n",
       "        63.73793265,  81.03429854, 259.09679683, 130.3296066 ,\n",
       "       136.72160239,  61.50560637,  74.02386366])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    " \n",
    "gbr = GradientBoostingRegressor(loss='absolute_error',\n",
    "                                learning_rate=0.1,\n",
    "                                n_estimators=100,\n",
    "                                max_depth = 5, \n",
    "                                random_state = 23,\n",
    "                                max_features = 5)\n",
    " \n",
    "gbr.fit(train_X, train_y)\n",
    "prediction = gbr.predict(test_X)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Square error: 56.87\n"
     ]
    }
   ],
   "source": [
    "test_rmse = mean_squared_error(test_y, prediction) ** (1 / 2)\n",
    "print('Root mean Square error: {:.2f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTREME GRADIENT BOOSTING MACHINE (XGBM)\n",
    "\n",
    "* Uses various regularization techniques.\n",
    "* follows parallel processing of each node. So its faster than GBM\n",
    "* Gets rid of worry abt missing values or sparse data.\n",
    "* Using trees helps them Handle large features.\n",
    "* They use CART (Which does not use all features at a time).\n",
    "* A model whose parameters adjust itself iteratively (XGBoost) will learn better from streaming data than one with a fixed set of parameters for the entire ensemble (RF).\n",
    "* Does not update weights of the samples post mis classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X,y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64), (1437,), (360,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state=42) \n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 3, 7, 2, 1, 5, 2, 5, 2, 1, 9, 4, 0, 4, 2, 3, 7, 8, 8, 4, 3,\n",
       "       9, 7, 5, 6, 3, 5, 6, 3, 4, 9, 1, 4, 4, 6, 9, 4, 7, 6, 6, 9, 1, 3,\n",
       "       6, 1, 3, 0, 6, 5, 5, 1, 9, 5, 6, 0, 9, 0, 0, 1, 0, 4, 5, 2, 4, 5,\n",
       "       7, 0, 7, 5, 9, 5, 5, 4, 7, 0, 1, 5, 5, 9, 9, 0, 2, 3, 8, 0, 6, 4,\n",
       "       4, 9, 1, 2, 8, 3, 5, 2, 9, 4, 4, 4, 4, 3, 5, 3, 1, 3, 5, 9, 4, 2,\n",
       "       7, 7, 4, 4, 1, 9, 2, 7, 8, 7, 2, 6, 9, 4, 0, 7, 2, 7, 5, 8, 7, 5,\n",
       "       7, 9, 0, 6, 6, 4, 2, 8, 0, 9, 4, 6, 9, 9, 6, 9, 0, 5, 5, 6, 6, 0,\n",
       "       6, 4, 3, 9, 3, 7, 7, 2, 9, 0, 4, 5, 8, 6, 5, 8, 9, 8, 4, 2, 1, 3,\n",
       "       7, 7, 2, 2, 3, 9, 8, 0, 3, 2, 2, 5, 6, 9, 9, 4, 1, 5, 4, 2, 3, 6,\n",
       "       4, 8, 5, 9, 5, 7, 1, 9, 4, 8, 1, 5, 4, 4, 9, 6, 1, 8, 6, 0, 4, 5,\n",
       "       2, 7, 4, 6, 4, 5, 6, 0, 3, 2, 3, 6, 7, 1, 5, 1, 4, 7, 6, 5, 8, 5,\n",
       "       5, 1, 5, 2, 8, 8, 9, 9, 7, 6, 2, 2, 2, 3, 4, 8, 8, 3, 6, 0, 9, 7,\n",
       "       7, 0, 1, 0, 4, 5, 1, 5, 3, 6, 0, 4, 1, 0, 0, 3, 6, 5, 9, 7, 3, 5,\n",
       "       5, 9, 9, 8, 5, 3, 3, 2, 0, 5, 8, 3, 4, 0, 2, 4, 6, 4, 3, 4, 5, 0,\n",
       "       5, 2, 1, 3, 1, 4, 1, 1, 7, 0, 1, 5, 2, 1, 2, 8, 7, 0, 6, 4, 8, 8,\n",
       "       5, 1, 8, 4, 5, 8, 7, 9, 8, 6, 0, 6, 2, 0, 7, 9, 8, 9, 5, 2, 7, 7,\n",
       "       1, 8, 7, 4, 3, 8, 3, 5])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(X_train,y_train)\n",
    "prediction = model.predict(X_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9694444444444444"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_test == prediction) / len(prediction)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting Machines (Light GBM)\n",
    "\n",
    "* Can handle huge amount of data without any issue.\n",
    "* Not good for lesser data\n",
    "* leaf-wise growth of the nodes of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATBOOST\n",
    "* Used for categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "* AdaBoost uses a different approach, where each subsequent model tries to focus on the samples that were misclassified by the previous model.\n",
    "* AdaBoost do not include these regularization techniques.\n",
    "* AdaBoost is generally slower than Gradient Boosting and XGBoost, as it requires multiple iterations to build the sequence of models.\n",
    "* AdaBoost require explicit imputation of missing values.\n",
    "* AdaBoost require a One-vs-All approach to solve multi-class problems.\n",
    "\n",
    "### AdaBoost\n",
    "* During each iteration in AdaBoost, the weights of incorrectly classified samples are increased, so that the next weak learner focuses more on these samples.\t\n",
    "* AdaBoost uses simple decision trees with one split known as the decision stumps of weak learners.\t\n",
    "* AdaBoost is more susceptible to noise and outliers in the data, as it assigns high weights to misclassified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 3, 8, 6, 5, 5, 3, 4, 8, 1, 9, 4, 0, 6, 2, 3, 7, 1, 8, 4, 3,\n",
       "       9, 7, 5, 6, 3, 5, 6, 3, 6, 9, 3, 6, 4, 6, 9, 4, 7, 6, 6, 9, 6, 3,\n",
       "       6, 1, 3, 0, 6, 5, 1, 1, 9, 5, 6, 0, 9, 0, 0, 1, 0, 4, 9, 5, 6, 5,\n",
       "       7, 0, 7, 1, 9, 5, 5, 4, 7, 0, 6, 5, 0, 0, 9, 4, 2, 3, 8, 0, 6, 4,\n",
       "       6, 9, 1, 2, 1, 3, 9, 8, 9, 4, 4, 4, 4, 3, 9, 3, 4, 1, 4, 9, 4, 8,\n",
       "       1, 7, 4, 6, 4, 0, 1, 8, 8, 3, 2, 6, 9, 9, 0, 6, 6, 7, 5, 3, 7, 5,\n",
       "       7, 9, 0, 6, 6, 4, 2, 8, 0, 9, 4, 6, 9, 9, 6, 0, 0, 1, 5, 6, 6, 0,\n",
       "       6, 4, 9, 9, 3, 8, 7, 6, 0, 0, 6, 5, 8, 6, 5, 8, 9, 8, 4, 2, 1, 3,\n",
       "       7, 7, 2, 2, 3, 0, 8, 0, 3, 2, 2, 5, 6, 9, 0, 4, 6, 6, 9, 2, 3, 6,\n",
       "       4, 8, 5, 9, 5, 7, 8, 9, 4, 2, 1, 5, 4, 4, 9, 6, 1, 8, 6, 0, 9, 4,\n",
       "       2, 4, 4, 6, 4, 6, 6, 7, 3, 2, 3, 6, 7, 9, 9, 1, 4, 4, 6, 9, 1, 5,\n",
       "       4, 8, 4, 8, 8, 1, 8, 8, 7, 6, 2, 8, 2, 3, 6, 8, 8, 3, 6, 0, 9, 7,\n",
       "       7, 0, 1, 0, 4, 5, 4, 5, 3, 6, 0, 4, 2, 0, 0, 3, 6, 5, 9, 7, 3, 5,\n",
       "       5, 9, 9, 8, 5, 3, 3, 2, 0, 4, 1, 3, 4, 0, 8, 4, 6, 4, 3, 4, 0, 0,\n",
       "       5, 2, 1, 3, 1, 4, 8, 4, 7, 0, 4, 1, 6, 8, 8, 8, 7, 0, 6, 4, 8, 8,\n",
       "       5, 1, 8, 4, 5, 8, 9, 9, 8, 6, 0, 1, 2, 0, 8, 9, 1, 0, 5, 5, 7, 7,\n",
       "       9, 2, 7, 6, 3, 8, 3, 5])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adagbm = AdaBoostClassifier(n_estimators=300, learning_rate=0.05, random_state=None, algorithm=\"SAMME\",)\n",
    "\n",
    "adagbm.fit(X_train, y_train)\n",
    "prediction = adagbm.predict(X_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7222222222222222"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_test == prediction) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
